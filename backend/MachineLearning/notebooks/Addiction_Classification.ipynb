{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89dead5a",
   "metadata": {},
   "source": [
    "Machine Learning - Leonardo Romariz\n",
    "\n",
    "Configurando o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effee14f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Imports necess√°rios\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# configura√ß√£o para n√£o exibir os warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Imports necess√°rios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea06ffa",
   "metadata": {},
   "source": [
    "## Carga do Dataset\n",
    "\n",
    "O dataset cont√©m informa√ß√µes sobre v√°rios estudantes, com cada registro representando uma pessoa. As vari√°veis incluem caracter√≠sticas demogr√°ficas, h√°bitos de uso de redes sociais, desempenho acad√™mico e indicadores de sa√∫de mental. A coluna Addicted_Score indica o n√≠vel de depend√™ncia do estudante em rela√ß√£o √†s redes sociais.\n",
    "\n",
    "Descri√ß√£o de Cada Coluna\n",
    "- Student_ID: Identificador √∫nico do estudante.\n",
    "- Age: Idade do estudante (anos).\n",
    "- Gender: G√™nero do estudante (0 = Feminino, 1 = Masculino, por exemplo).\n",
    "- Academic_Level: N√≠vel acad√™mico do estudante.\n",
    "- Country: Pa√≠s de origem do estudante.\n",
    "- Avg_Daily_Usage_Hours: M√©dia de horas di√°rias de uso de redes sociais.\n",
    "- Most_Used_Platform: Plataforma de rede social mais utilizada.\n",
    "- Affects_Academic_Performance: Indica se o uso de redes sociais afeta o desempenho acad√™mico.\n",
    "- Sleep_Hours_Per_Night: M√©dia de horas de sono por noite.\n",
    "- Mental_Health_Score: Pontua√ß√£o relacionada √† sa√∫de mental do estudante.\n",
    "- Relationship_Status: Status de relacionamento do estudante.\n",
    "- Conflicts_Over_Social_Media: Indica se h√° conflitos devido ao uso de redes sociais.\n",
    "- Addicted_Score: Pontua√ß√£o de depend√™ncia em redes sociais.\n",
    "\n",
    "O objetivo √© analisar e prever o n√≠vel de depend√™ncia dos estudantes em rela√ß√£o √†s redes sociais com base em suas caracter√≠sticas e h√°bitos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdad839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa a URL de importa√ß√£o do dataset\n",
    "url = r\"backend\\MachineLearning\\data\\students_social_media_addiction_processed.csv\"\n",
    "\n",
    "# L√™ o arquivo \n",
    "dataset = pd.read_csv(url, delimiter=',')\n",
    "\n",
    "# Mostra as primeiras linhas do dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11129f3c",
   "metadata": {},
   "source": [
    "## Separa√ß√£o em conjunto de treino e conjunto de teste com holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.20 # tamanho do conjunto de teste\n",
    "seed = 7 # semente aleat√≥ria\n",
    "\n",
    "# Separa√ß√£o em conjuntos de treino e teste\n",
    "array = dataset.values\n",
    "X = array[:,1:11]\n",
    "y = array[:,11]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "    test_size=test_size, shuffle=True, random_state=seed, stratify=y) # holdout com estratifica√ß√£o\n",
    "\n",
    "# Par√¢metros e parti√ß√µes da valida√ß√£o cruzada\n",
    "scoring = 'accuracy'\n",
    "num_particoes = 10\n",
    "kfold = StratifiedKFold(n_splits=num_particoes, shuffle=True, random_state=seed) # valida√ß√£o cruzada com estratifica√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1314cf4",
   "metadata": {},
   "source": [
    "## Modelagem e Infer√™ncia\n",
    "\n",
    "### Cria√ß√£o e avalia√ß√£o de modelos: linha base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec063f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7) # definindo uma semente global\n",
    "\n",
    "# Lista que armazenar√° os modelos\n",
    "models = []\n",
    "\n",
    "# Criando os modelos e adicionando-os na lista de modelos\n",
    "models.append(('LR', LogisticRegression(max_iter=200))) \n",
    "models.append(('KNN', KNeighborsClassifier())) \n",
    "models.append(('CART', DecisionTreeClassifier())) \n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# Definindo os par√¢metros do classificador base para o BaggingClassifier\n",
    "base = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "# Criando os modelos para o VotingClassifier\n",
    "bases = []\n",
    "model1 = LogisticRegression(max_iter=200)\n",
    "bases.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "bases.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "bases.append(('svm', model3))\n",
    "\n",
    "# Criando os ensembles e adicionando-os na lista de modelos\n",
    "models.append(('Bagging', BaggingClassifier(estimator=base, n_estimators=num_trees)))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))\n",
    "models.append(('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)))\n",
    "models.append(('Ada', AdaBoostClassifier(n_estimators=num_trees)))\n",
    "models.append(('GB', GradientBoostingClassifier(n_estimators=num_trees)))\n",
    "models.append(('Voting', VotingClassifier(bases)))\n",
    "\n",
    "# Listas para armazenar os resultados\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "# Avalia√ß√£o dos modelos (treinamento)\n",
    "for name, model in models:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# Boxplot de compara√ß√£o dos modelos\n",
    "fig = plt.figure(figsize=(15,10)) \n",
    "fig.suptitle('Compara√ß√£o dos Modelos') \n",
    "ax = fig.add_subplot(111) \n",
    "plt.boxplot(results) \n",
    "ax.set_xticklabels(names) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a542aa6b",
   "metadata": {},
   "source": [
    "### Cria√ß√£o e avalia√ß√£o de modelos: dados padronizados e normalizados\n",
    "\n",
    "Trabalhando com dados pontencialmente desbalanceados ou sens√≠veis a escala\n",
    "\n",
    "StandardScaler (padroniza√ß√£o do conjunto de dados) e MinMaxScaler (normaliza√ß√£o do conjunto de dados) s√£o duas t√©cnicas de normaliza√ß√£o/escala usadas em machine learning para pr√©-processamento de dados e s√£o √∫teis para preparar dados para algoritmos de aprendizado de m√°quina que s√£o sens√≠veis √† escala dos dados.\n",
    "\n",
    "##### StandardScaler\n",
    "StandardScaler padroniza os dados, ou seja, remove a m√©dia e escala os dados para que tenham uma vari√¢ncia unit√°ria. Ele transforma os dados para que a m√©dia de cada feature seja 0 e a vari√¢ncia seja 1.\n",
    "\n",
    "F√≥rmula: $z_i=\\frac{x_i-\\mu}{\\sigma}$\n",
    "\n",
    " \n",
    "onde:\n",
    "- $x_i$ √© o valor original do $i$-√©simo termo da feature.\n",
    "- $\\mu$ √© a m√©dia dos valores da feature.\n",
    "- $\\sigma$ √© o desvio padr√£o dos valores da feature.\n",
    "ùë•\n",
    "x √© o valor original da feature.\n",
    "ùúá\n",
    "Œº √© a m√©dia dos valores da feature.\n",
    "ùúé\n",
    "œÉ √© o desvio padr√£o dos valores da feature.\n",
    "\n",
    "\n",
    "##### MinMaxScaler\n",
    "MinMaxScaler escala e transforma os dados para um intervalo espec√≠fico, geralmente entre 0 e 1. Ele transforma os dados para que o menor valor de uma feature seja 0 e o maior valor seja 1.\n",
    "\n",
    "F√≥rmula: $z_i=\\frac{x_i-min(x)}{max(x)-min(x)}$\n",
    "\n",
    "onde:\n",
    "- $x_i$ √© o valor original do $i$-√©simo termo da feature.\n",
    "- $min(x)$ √© o menor valor da feature.\n",
    "- $max(x)$ √© o maior valor da feature.\n",
    "\n",
    "N√≥s vamos aplicar essas t√©cnicas para os dados do dataset de v√≠cio em redes sociais atrav√©s da constru√ß√£o de pipelines. Pipelines s√£o uma maneira de simplificar o processo de constru√ß√£o de modelos, permitindo que voc√™ execute v√°rias etapas de pr√©-processamento e modelagem em sequ√™ncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16958005",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7) # definindo uma semente global para este bloco\n",
    "\n",
    "# Listas para armazenar os armazenar os pipelines e os resultados para todas as vis√µes do dataset\n",
    "pipelines = []\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "\n",
    "# Criando os elementos do pipeline\n",
    "\n",
    "# Algoritmos que ser√£o utilizados\n",
    "reg_log = ('LR', LogisticRegression(max_iter=200))\n",
    "knn = ('KNN', KNeighborsClassifier())\n",
    "cart = ('CART', DecisionTreeClassifier())\n",
    "naive_bayes = ('NB', GaussianNB())\n",
    "svm = ('SVM', SVC())\n",
    "bagging = ('Bag', BaggingClassifier(estimator=base, n_estimators=num_trees))\n",
    "random_forest = ('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features))\n",
    "extra_trees = ('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features))\n",
    "adaboost = ('Ada', AdaBoostClassifier(n_estimators=num_trees))\n",
    "gradient_boosting = ('GB', GradientBoostingClassifier(n_estimators=num_trees))\n",
    "voting = ('Voting', VotingClassifier(bases))\n",
    "\n",
    "# Transforma√ß√µes que ser√£o utilizadas\n",
    "standard_scaler = ('StandardScaler', StandardScaler())\n",
    "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
    "\n",
    "\n",
    "# Montando os pipelines\n",
    "# A ordem de execu√ß√£o √© da esquerda para a direita.\n",
    "\n",
    "# Dataset original\n",
    "pipelines.append(('LR-orig', Pipeline([reg_log]))) \n",
    "pipelines.append(('KNN-orig', Pipeline([knn])))\n",
    "pipelines.append(('CART-orig', Pipeline([cart])))\n",
    "pipelines.append(('NB-orig', Pipeline([naive_bayes])))\n",
    "pipelines.append(('SVM-orig', Pipeline([svm])))\n",
    "pipelines.append(('Bag-orig', Pipeline([bagging])))\n",
    "pipelines.append(('RF-orig', Pipeline([random_forest])))\n",
    "pipelines.append(('ET-orig', Pipeline([extra_trees])))\n",
    "pipelines.append(('Ada-orig', Pipeline([adaboost])))\n",
    "pipelines.append(('GB-orig', Pipeline([gradient_boosting])))\n",
    "pipelines.append(('Vot-orig', Pipeline([voting])))\n",
    "\n",
    "# Dataset Padronizado\n",
    "pipelines.append(('LR-padr', Pipeline([standard_scaler, reg_log]))) \n",
    "pipelines.append(('KNN-padr', Pipeline([standard_scaler, knn])))\n",
    "pipelines.append(('CART-padr', Pipeline([standard_scaler, cart])))\n",
    "pipelines.append(('NB-padr', Pipeline([standard_scaler, naive_bayes])))\n",
    "pipelines.append(('SVM-padr', Pipeline([standard_scaler, svm])))\n",
    "pipelines.append(('Bag-padr', Pipeline([standard_scaler, bagging]))) \n",
    "pipelines.append(('RF-padr', Pipeline([standard_scaler, random_forest])))\n",
    "pipelines.append(('ET-padr', Pipeline([standard_scaler, extra_trees])))\n",
    "pipelines.append(('Ada-padr', Pipeline([standard_scaler, adaboost])))\n",
    "pipelines.append(('GB-padr', Pipeline([standard_scaler, gradient_boosting])))\n",
    "pipelines.append(('Vot-padr', Pipeline([standard_scaler, voting])))\n",
    "\n",
    "# Dataset Normalizado\n",
    "pipelines.append(('LR-norm', Pipeline([min_max_scaler, reg_log]))) \n",
    "pipelines.append(('KNN-norm', Pipeline([min_max_scaler, knn])))\n",
    "pipelines.append(('CART-norm', Pipeline([min_max_scaler, cart])))\n",
    "pipelines.append(('NB-norm', Pipeline([min_max_scaler, naive_bayes])))\n",
    "pipelines.append(('SVM-norm', Pipeline([min_max_scaler, svm])))\n",
    "pipelines.append(('Bag-norm', Pipeline([min_max_scaler, bagging]))) \n",
    "pipelines.append(('RF-norm', Pipeline([min_max_scaler, random_forest])))\n",
    "pipelines.append(('ET-norm', Pipeline([min_max_scaler, extra_trees])))\n",
    "pipelines.append(('Ada-norm', Pipeline([min_max_scaler, adaboost])))\n",
    "pipelines.append(('GB-norm', Pipeline([min_max_scaler, gradient_boosting])))\n",
    "pipelines.append(('Vot-norm', Pipeline([min_max_scaler, voting])))\n",
    "\n",
    "# Executando os pipelines\n",
    "for name, model in pipelines:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %.3f (%.3f)\" % (name, cv_results.mean(), cv_results.std()) # formatando para 3 casas decimais\n",
    "    print(msg)\n",
    "\n",
    "# Boxplot de compara√ß√£o dos modelos\n",
    "fig = plt.figure(figsize=(25,6))\n",
    "fig.suptitle('Compara√ß√£o dos Modelos - Dataset orginal, padronizado e normalizado') \n",
    "ax = fig.add_subplot(111) \n",
    "plt.boxplot(results) \n",
    "ax.set_xticklabels(names, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c00b70",
   "metadata": {},
   "source": [
    "### Otimiza√ß√£o dos hiperpar√¢metros\n",
    "\n",
    "A otimiza√ß√£o de hiperpar√¢metros √© o processo de encontrar os valores ideais para os hiperpar√¢metros de um modelo de machine learning. O objetivo √© encontrar a combina√ß√£o de hiperpar√¢metros que resulta no melhor desempenho do modelo.\n",
    "\n",
    "\n",
    "##### Grid Search (*for√ßa bruta*)\n",
    "\n",
    "Como Funciona o Grid Search?\n",
    "1. Defini√ß√£o do Espa√ßo de Hiperpar√¢metros: Primeiro, define-se um conjunto de valores poss√≠veis para cada hiperpar√¢metro.\n",
    "2. Avalia√ß√£o das Combina√ß√µes: Em seguida, cada combina√ß√£o poss√≠vel desses valores √© avaliada.\n",
    "3. Sele√ß√£o do Melhor Conjunto: A combina√ß√£o de hiperpar√¢metros que produz o melhor desempenho √© selecionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)  # Definindo uma semente global para este bloco\n",
    "\n",
    "# Lista de modelos\n",
    "models = []\n",
    "\n",
    "# Criando os modelos e adicionando-os na lista de modelos\n",
    "models.append(('LR', LogisticRegression(max_iter=200))) \n",
    "models.append(('KNN', KNeighborsClassifier())) \n",
    "models.append(('CART', DecisionTreeClassifier())) \n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# Definindo os par√¢metros do classificador base para o BaggingClassifier\n",
    "base = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "# Criando os modelos para o VotingClassifier\n",
    "bases = []\n",
    "model1 = LogisticRegression(max_iter=200)\n",
    "bases.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "bases.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "bases.append(('svm', model3))\n",
    "\n",
    "# Criando os ensembles e adicionando-os na lista de modelos\n",
    "models.append(('Bagging', BaggingClassifier(estimator=base, n_estimators=num_trees)))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))\n",
    "models.append(('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)))\n",
    "models.append(('Ada', AdaBoostClassifier(n_estimators=num_trees)))\n",
    "models.append(('GB', GradientBoostingClassifier(n_estimators=num_trees)))\n",
    "models.append(('Voting', VotingClassifier(estimators=bases, voting='hard')))\n",
    "\n",
    "# Definindo os componentes do pipeline\n",
    "standard_scaler = ('StandardScaler', StandardScaler())\n",
    "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
    "\n",
    "# Lista de pipelines\n",
    "pipelines = []\n",
    "\n",
    "# Criando pipelines para cada modelo\n",
    "for name, model in models:\n",
    "    pipelines.append((name + '-orig', Pipeline(steps=[(name, model)])))\n",
    "    pipelines.append((name + '-padr', Pipeline(steps=[standard_scaler, (name, model)])))\n",
    "    pipelines.append((name + '-norm', Pipeline(steps=[min_max_scaler, (name, model)])))\n",
    "\n",
    "# Definindo os par√¢metros para GridSearchCV\n",
    "param_grids = {\n",
    "    'LR': {\n",
    "        'LR__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'LR__solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'KNN': {\n",
    "        'KNN__n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21],\n",
    "        'KNN__metric': [\"euclidean\", \"manhattan\", \"minkowski\"]\n",
    "    },\n",
    "    'CART': {\n",
    "        'CART__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "        'CART__min_samples_split': [2, 5, 10],\n",
    "        'CART__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'NB': {\n",
    "        'NB__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'SVM__C': [0.1, 1, 10, 100],\n",
    "        'SVM__gamma': [1, 0.1, 0.01, 0.001],\n",
    "        'SVM__kernel': ['rbf', 'linear']\n",
    "    },\n",
    "    'RF': {\n",
    "        'RF__n_estimators': [10, 50, 100, 200],\n",
    "        'RF__max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'RF__max_depth': [None, 10, 20, 30],\n",
    "        'RF__min_samples_split': [2, 5, 10],\n",
    "        'RF__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'ET': {\n",
    "        'ET__n_estimators': [10, 50, 100, 200],\n",
    "        'ET__max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'ET__max_depth': [None, 10, 20, 30],\n",
    "        'ET__min_samples_split': [2, 5, 10],\n",
    "        'ET__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Ada': {\n",
    "        'Ada__n_estimators': [10, 50, 100, 200],\n",
    "        'Ada__learning_rate': [0.01, 0.1, 1, 10]\n",
    "    },\n",
    "    'GB': {\n",
    "        'GB__n_estimators': [10, 50, 100, 200],\n",
    "        'GB__learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "        'GB__max_depth': [3, 5, 7, 9]\n",
    "    },\n",
    "    'Voting': {\n",
    "        # Para VotingClassifier, geralmente n√£o h√° hiperpar√¢metros para ajustar diretamente\n",
    "        # Ajustar os hiperpar√¢metros dos estimadores base individualmente se necess√°rio\n",
    "    }\n",
    "}\n",
    "\n",
    "# Par√¢metros de cross-validation e scoring\n",
    "scoring = 'accuracy'\n",
    "kfold = 5\n",
    "\n",
    "# Executando o GridSearchCV para cada pipeline\n",
    "for name, pipeline in pipelines:\n",
    "    model_type = name.split('-')[0]\n",
    "    if model_type in param_grids:\n",
    "        param_grid = param_grids[model_type]\n",
    "    else:\n",
    "        param_grid = {}  # Para modelos que n√£o t√™m par√¢metros definidos\n",
    "\n",
    "    grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "    grid.fit(X_train, y_train)\n",
    "    # Imprimindo a melhor configura√ß√£o\n",
    "    print(\"Modelo: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_))\n",
    "    \n",
    "#Demora para rodar, mas √© poss√≠vel salvar o modelo treinado para uso posterior\n",
    "# with open('best_model.pkl', 'wb') as f:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning do KNN\n",
    "\n",
    "np.random.seed(7) # definindo uma semente global para este bloco\n",
    "\n",
    "pipelines = []\n",
    "\n",
    "# Definindo os componentes do pipeline\n",
    "knn = ('KNN', KNeighborsClassifier())\n",
    "standard_scaler = ('StandardScaler', StandardScaler())\n",
    "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
    "\n",
    "pipelines.append(('knn-orig', Pipeline(steps=[knn])))\n",
    "pipelines.append(('knn-padr', Pipeline(steps=[standard_scaler, knn])))\n",
    "pipelines.append(('knn-norm', Pipeline(steps=[min_max_scaler, knn])))\n",
    "\n",
    "param_grid = {\n",
    "    'KNN__n_neighbors': [1,3,5,7,9,11,13,15,17,19,21],\n",
    "    'KNN__metric': [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "}\n",
    "\n",
    "# Prepara e executa o GridSearchCV\n",
    "for name, model in pipelines:    \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "    grid.fit(X_train, y_train)\n",
    "    # imprime a melhor configura√ß√£o\n",
    "    print(\"Sem tratamento de missings: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050de15",
   "metadata": {},
   "source": [
    "## Finaliza√ß√£o do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia√ß√£o do modelo com o conjunto de testes\n",
    "# Melhor modelo\n",
    "# RF-norm - Melhor: 0.783287 usando {'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__min_samples_leaf': 1, 'RF__min_samples_split': 2, 'RF__n_estimators': 50}\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "# Prepara√ß√£o do modelo\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train) # aplica√ß√£o da normaliza√ß√£o no conjunto de treino\n",
    "model = RandomForestClassifier(n_estimators=50, \n",
    "                               max_features='sqrt',\n",
    "                               min_samples_split=2,\n",
    "                               max_depth=10,\n",
    "                               min_samples_leaf=1)\n",
    "model.fit(rescaledX, y_train)\n",
    "\n",
    "# Estimativa da acur√°cia no conjunto de teste\n",
    "rescaledTestX = scaler.transform(X_test) # aplica√ß√£o da normaliza√ß√£o no conjunto de teste\n",
    "predictions = model.predict(rescaledTestX)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739f4bf",
   "metadata": {},
   "source": [
    "Rodando o modelo a partir de um pipeline com os hiperpar√¢metros otimizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=50, \n",
    "                               max_features='sqrt',\n",
    "                               min_samples_split=2,\n",
    "                               max_depth=10,\n",
    "                               min_samples_leaf=1)\n",
    "\n",
    "pipeline = Pipeline(steps=[('MinMaxScaler', MinMaxScaler()), ('RF', model)])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3bee5",
   "metadata": {},
   "source": [
    "## Salvando os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c610c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo\n",
    "model_filename = 'rf_diabetes_classifier.pkl'\n",
    "with open(\"../models/\"+model_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Salvando o scaler\n",
    "scaler_filename = 'minmax_scaler_diabetes.pkl'\n",
    "with open(\"../scalers/\"+scaler_filename, 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "    \n",
    "# Salvando o pipeline\n",
    "pipeline_filename = 'rf_diabetes_pipeline.pkl'\n",
    "with open(\"../pipelines/\"+pipeline_filename, 'wb') as file:\n",
    "    pickle.dump(pipeline, file)\n",
    "    \n",
    "    \n",
    "# Salvando X_test e y_test\n",
    "X_test_df = pd.DataFrame(X_test, columns=dataset.columns[:-1])\n",
    "y_test_df = pd.DataFrame(y_test, columns=[dataset.columns[-1]])\n",
    "X_test_df.to_csv(\"../data/addicted_score.csv\", index=False)\n",
    "y_test_df.to_csv(\"../data/students_social_media_data_sem_addicted_score.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988be820",
   "metadata": {},
   "source": [
    "## Simulando a aplica√ß√£o do modelo em dados n√£o vistos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02678e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara√ß√£o do modelo com TODO o dataset\n",
    "scaler = MinMaxScaler().fit(X) # ajuste do scaler com TODO o dataset\n",
    "rescaledX = scaler.transform(X) # aplica√ß√£o da normaliza√ß√£o com TODO o dataset\n",
    "model.fit(rescaledX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novos dados - n√£o sabemos a classe!\n",
    "data = {'preg':  [1, 9, 5],\n",
    "        'plas': [90, 100, 110],\n",
    "        'pres': [50, 60, 50],\n",
    "        'skin': [30, 30, 30],\n",
    "        'test': [100, 100, 100],\n",
    "        'mass': [20.0, 30.0, 40.0],\n",
    "        'pedi': [1.0, 2.0, 1.0],\n",
    "        'age': [15, 40, 40],  \n",
    "        }\n",
    "\n",
    "atributos = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
    "entrada = pd.DataFrame(data, columns=atributos)\n",
    "\n",
    "array_entrada = entrada.values\n",
    "X_entrada = array_entrada[:,0:8].astype(float)\n",
    "\n",
    "# Padroniza√ß√£o nos dados de entrada usando o scaler utilizado em X\n",
    "rescaledEntradaX = scaler.transform(X_entrada)\n",
    "print(rescaledEntradaX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9605f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predi√ß√£o de classes dos dados de entrada\n",
    "saidas = model.predict(rescaledEntradaX)\n",
    "print(saidas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
