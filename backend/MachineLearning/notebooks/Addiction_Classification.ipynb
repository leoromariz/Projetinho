{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89dead5a",
   "metadata": {},
   "source": [
    "Machine Learning - Leonardo Romariz\n",
    "\n",
    "Configurando o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effee14f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Imports necessários\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# configuração para não exibir os warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Imports necessários\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea06ffa",
   "metadata": {},
   "source": [
    "## Carga do Dataset\n",
    "\n",
    "O dataset contém informações sobre vários estudantes, com cada registro representando uma pessoa. As variáveis incluem características demográficas, hábitos de uso de redes sociais, desempenho acadêmico e indicadores de saúde mental. A coluna Addicted_Score indica o nível de dependência do estudante em relação às redes sociais.\n",
    "\n",
    "Descrição de Cada Coluna\n",
    "- Student_ID: Identificador único do estudante.\n",
    "- Age: Idade do estudante (anos).\n",
    "- Gender: Gênero do estudante (0 = Feminino, 1 = Masculino, por exemplo).\n",
    "- Academic_Level: Nível acadêmico do estudante.\n",
    "- Country: País de origem do estudante.\n",
    "- Avg_Daily_Usage_Hours: Média de horas diárias de uso de redes sociais.\n",
    "- Most_Used_Platform: Plataforma de rede social mais utilizada.\n",
    "- Affects_Academic_Performance: Indica se o uso de redes sociais afeta o desempenho acadêmico.\n",
    "- Sleep_Hours_Per_Night: Média de horas de sono por noite.\n",
    "- Mental_Health_Score: Pontuação relacionada à saúde mental do estudante.\n",
    "- Relationship_Status: Status de relacionamento do estudante.\n",
    "- Conflicts_Over_Social_Media: Indica se há conflitos devido ao uso de redes sociais.\n",
    "- Addicted_Score: Pontuação de dependência em redes sociais.\n",
    "\n",
    "O objetivo é analisar e prever o nível de dependência dos estudantes em relação às redes sociais com base em suas características e hábitos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdad839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa a URL de importação do dataset\n",
    "url = r\"backend\\MachineLearning\\data\\students_social_media_addiction_processed.csv\"\n",
    "\n",
    "# Lê o arquivo \n",
    "dataset = pd.read_csv(url, delimiter=',')\n",
    "\n",
    "# Mostra as primeiras linhas do dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11129f3c",
   "metadata": {},
   "source": [
    "## Separação em conjunto de treino e conjunto de teste com holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.20 # tamanho do conjunto de teste\n",
    "seed = 7 # semente aleatória\n",
    "\n",
    "# Separação em conjuntos de treino e teste\n",
    "array = dataset.values\n",
    "X = array[:,1:11]\n",
    "y = array[:,11]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "    test_size=test_size, shuffle=True, random_state=seed, stratify=y) # holdout com estratificação\n",
    "\n",
    "# Parâmetros e partições da validação cruzada\n",
    "scoring = 'accuracy'\n",
    "num_particoes = 10\n",
    "kfold = StratifiedKFold(n_splits=num_particoes, shuffle=True, random_state=seed) # validação cruzada com estratificação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1314cf4",
   "metadata": {},
   "source": [
    "## Modelagem e Inferência\n",
    "\n",
    "### Criação e avaliação de modelos: linha base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec063f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7) # definindo uma semente global\n",
    "\n",
    "# Lista que armazenará os modelos\n",
    "models = []\n",
    "\n",
    "# Criando os modelos e adicionando-os na lista de modelos\n",
    "models.append(('LR', LogisticRegression(max_iter=200))) \n",
    "models.append(('KNN', KNeighborsClassifier())) \n",
    "models.append(('CART', DecisionTreeClassifier())) \n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# Definindo os parâmetros do classificador base para o BaggingClassifier\n",
    "base = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "# Criando os modelos para o VotingClassifier\n",
    "bases = []\n",
    "model1 = LogisticRegression(max_iter=200)\n",
    "bases.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "bases.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "bases.append(('svm', model3))\n",
    "\n",
    "# Criando os ensembles e adicionando-os na lista de modelos\n",
    "models.append(('Bagging', BaggingClassifier(estimator=base, n_estimators=num_trees)))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))\n",
    "models.append(('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)))\n",
    "models.append(('Ada', AdaBoostClassifier(n_estimators=num_trees)))\n",
    "models.append(('GB', GradientBoostingClassifier(n_estimators=num_trees)))\n",
    "models.append(('Voting', VotingClassifier(bases)))\n",
    "\n",
    "# Listas para armazenar os resultados\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "# Avaliação dos modelos (treinamento)\n",
    "for name, model in models:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# Boxplot de comparação dos modelos\n",
    "fig = plt.figure(figsize=(15,10)) \n",
    "fig.suptitle('Comparação dos Modelos') \n",
    "ax = fig.add_subplot(111) \n",
    "plt.boxplot(results) \n",
    "ax.set_xticklabels(names) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a542aa6b",
   "metadata": {},
   "source": [
    "### Criação e avaliação de modelos: dados padronizados e normalizados\n",
    "\n",
    "Trabalhando com dados pontencialmente desbalanceados ou sensíveis a escala\n",
    "\n",
    "StandardScaler (padronização do conjunto de dados) e MinMaxScaler (normalização do conjunto de dados) são duas técnicas de normalização/escala usadas em machine learning para pré-processamento de dados e são úteis para preparar dados para algoritmos de aprendizado de máquina que são sensíveis à escala dos dados.\n",
    "\n",
    "##### StandardScaler\n",
    "StandardScaler padroniza os dados, ou seja, remove a média e escala os dados para que tenham uma variância unitária. Ele transforma os dados para que a média de cada feature seja 0 e a variância seja 1.\n",
    "\n",
    "Fórmula: $z_i=\\frac{x_i-\\mu}{\\sigma}$\n",
    "\n",
    " \n",
    "onde:\n",
    "- $x_i$ é o valor original do $i$-ésimo termo da feature.\n",
    "- $\\mu$ é a média dos valores da feature.\n",
    "- $\\sigma$ é o desvio padrão dos valores da feature.\n",
    "𝑥\n",
    "x é o valor original da feature.\n",
    "𝜇\n",
    "μ é a média dos valores da feature.\n",
    "𝜎\n",
    "σ é o desvio padrão dos valores da feature.\n",
    "\n",
    "\n",
    "##### MinMaxScaler\n",
    "MinMaxScaler escala e transforma os dados para um intervalo específico, geralmente entre 0 e 1. Ele transforma os dados para que o menor valor de uma feature seja 0 e o maior valor seja 1.\n",
    "\n",
    "Fórmula: $z_i=\\frac{x_i-min(x)}{max(x)-min(x)}$\n",
    "\n",
    "onde:\n",
    "- $x_i$ é o valor original do $i$-ésimo termo da feature.\n",
    "- $min(x)$ é o menor valor da feature.\n",
    "- $max(x)$ é o maior valor da feature.\n",
    "\n",
    "Nós vamos aplicar essas técnicas para os dados do dataset de vício em redes sociais através da construção de pipelines. Pipelines são uma maneira de simplificar o processo de construção de modelos, permitindo que você execute várias etapas de pré-processamento e modelagem em sequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16958005",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7) # definindo uma semente global para este bloco\n",
    "\n",
    "# Listas para armazenar os armazenar os pipelines e os resultados para todas as visões do dataset\n",
    "pipelines = []\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "\n",
    "# Criando os elementos do pipeline\n",
    "\n",
    "# Algoritmos que serão utilizados\n",
    "reg_log = ('LR', LogisticRegression(max_iter=200))\n",
    "knn = ('KNN', KNeighborsClassifier())\n",
    "cart = ('CART', DecisionTreeClassifier())\n",
    "naive_bayes = ('NB', GaussianNB())\n",
    "svm = ('SVM', SVC())\n",
    "bagging = ('Bag', BaggingClassifier(estimator=base, n_estimators=num_trees))\n",
    "random_forest = ('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features))\n",
    "extra_trees = ('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features))\n",
    "adaboost = ('Ada', AdaBoostClassifier(n_estimators=num_trees))\n",
    "gradient_boosting = ('GB', GradientBoostingClassifier(n_estimators=num_trees))\n",
    "voting = ('Voting', VotingClassifier(bases))\n",
    "\n",
    "# Transformações que serão utilizadas\n",
    "standard_scaler = ('StandardScaler', StandardScaler())\n",
    "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
    "\n",
    "\n",
    "# Montando os pipelines\n",
    "# A ordem de execução é da esquerda para a direita.\n",
    "\n",
    "# Dataset original\n",
    "pipelines.append(('LR-orig', Pipeline([reg_log]))) \n",
    "pipelines.append(('KNN-orig', Pipeline([knn])))\n",
    "pipelines.append(('CART-orig', Pipeline([cart])))\n",
    "pipelines.append(('NB-orig', Pipeline([naive_bayes])))\n",
    "pipelines.append(('SVM-orig', Pipeline([svm])))\n",
    "pipelines.append(('Bag-orig', Pipeline([bagging])))\n",
    "pipelines.append(('RF-orig', Pipeline([random_forest])))\n",
    "pipelines.append(('ET-orig', Pipeline([extra_trees])))\n",
    "pipelines.append(('Ada-orig', Pipeline([adaboost])))\n",
    "pipelines.append(('GB-orig', Pipeline([gradient_boosting])))\n",
    "pipelines.append(('Vot-orig', Pipeline([voting])))\n",
    "\n",
    "# Dataset Padronizado\n",
    "pipelines.append(('LR-padr', Pipeline([standard_scaler, reg_log]))) \n",
    "pipelines.append(('KNN-padr', Pipeline([standard_scaler, knn])))\n",
    "pipelines.append(('CART-padr', Pipeline([standard_scaler, cart])))\n",
    "pipelines.append(('NB-padr', Pipeline([standard_scaler, naive_bayes])))\n",
    "pipelines.append(('SVM-padr', Pipeline([standard_scaler, svm])))\n",
    "pipelines.append(('Bag-padr', Pipeline([standard_scaler, bagging]))) \n",
    "pipelines.append(('RF-padr', Pipeline([standard_scaler, random_forest])))\n",
    "pipelines.append(('ET-padr', Pipeline([standard_scaler, extra_trees])))\n",
    "pipelines.append(('Ada-padr', Pipeline([standard_scaler, adaboost])))\n",
    "pipelines.append(('GB-padr', Pipeline([standard_scaler, gradient_boosting])))\n",
    "pipelines.append(('Vot-padr', Pipeline([standard_scaler, voting])))\n",
    "\n",
    "# Dataset Normalizado\n",
    "pipelines.append(('LR-norm', Pipeline([min_max_scaler, reg_log]))) \n",
    "pipelines.append(('KNN-norm', Pipeline([min_max_scaler, knn])))\n",
    "pipelines.append(('CART-norm', Pipeline([min_max_scaler, cart])))\n",
    "pipelines.append(('NB-norm', Pipeline([min_max_scaler, naive_bayes])))\n",
    "pipelines.append(('SVM-norm', Pipeline([min_max_scaler, svm])))\n",
    "pipelines.append(('Bag-norm', Pipeline([min_max_scaler, bagging]))) \n",
    "pipelines.append(('RF-norm', Pipeline([min_max_scaler, random_forest])))\n",
    "pipelines.append(('ET-norm', Pipeline([min_max_scaler, extra_trees])))\n",
    "pipelines.append(('Ada-norm', Pipeline([min_max_scaler, adaboost])))\n",
    "pipelines.append(('GB-norm', Pipeline([min_max_scaler, gradient_boosting])))\n",
    "pipelines.append(('Vot-norm', Pipeline([min_max_scaler, voting])))\n",
    "\n",
    "# Executando os pipelines\n",
    "for name, model in pipelines:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %.3f (%.3f)\" % (name, cv_results.mean(), cv_results.std()) # formatando para 3 casas decimais\n",
    "    print(msg)\n",
    "\n",
    "# Boxplot de comparação dos modelos\n",
    "fig = plt.figure(figsize=(25,6))\n",
    "fig.suptitle('Comparação dos Modelos - Dataset orginal, padronizado e normalizado') \n",
    "ax = fig.add_subplot(111) \n",
    "plt.boxplot(results) \n",
    "ax.set_xticklabels(names, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c00b70",
   "metadata": {},
   "source": [
    "### Otimização dos hiperparâmetros\n",
    "\n",
    "A otimização de hiperparâmetros é o processo de encontrar os valores ideais para os hiperparâmetros de um modelo de machine learning. O objetivo é encontrar a combinação de hiperparâmetros que resulta no melhor desempenho do modelo.\n",
    "\n",
    "\n",
    "##### Grid Search (*força bruta*)\n",
    "\n",
    "Como Funciona o Grid Search?\n",
    "1. Definição do Espaço de Hiperparâmetros: Primeiro, define-se um conjunto de valores possíveis para cada hiperparâmetro.\n",
    "2. Avaliação das Combinações: Em seguida, cada combinação possível desses valores é avaliada.\n",
    "3. Seleção do Melhor Conjunto: A combinação de hiperparâmetros que produz o melhor desempenho é selecionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)  # Definindo uma semente global para este bloco\n",
    "\n",
    "# Lista de modelos\n",
    "models = []\n",
    "\n",
    "# Criando os modelos e adicionando-os na lista de modelos\n",
    "models.append(('LR', LogisticRegression(max_iter=200))) \n",
    "models.append(('KNN', KNeighborsClassifier())) \n",
    "models.append(('CART', DecisionTreeClassifier())) \n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# Definindo os parâmetros do classificador base para o BaggingClassifier\n",
    "base = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "# Criando os modelos para o VotingClassifier\n",
    "bases = []\n",
    "model1 = LogisticRegression(max_iter=200)\n",
    "bases.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "bases.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "bases.append(('svm', model3))\n",
    "\n",
    "# Criando os ensembles e adicionando-os na lista de modelos\n",
    "models.append(('Bagging', BaggingClassifier(estimator=base, n_estimators=num_trees)))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))\n",
    "models.append(('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)))\n",
    "models.append(('Ada', AdaBoostClassifier(n_estimators=num_trees)))\n",
    "models.append(('GB', GradientBoostingClassifier(n_estimators=num_trees)))\n",
    "models.append(('Voting', VotingClassifier(estimators=bases, voting='hard')))\n",
    "\n",
    "# Definindo os componentes do pipeline\n",
    "standard_scaler = ('StandardScaler', StandardScaler())\n",
    "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
    "\n",
    "# Lista de pipelines\n",
    "pipelines = []\n",
    "\n",
    "# Criando pipelines para cada modelo\n",
    "for name, model in models:\n",
    "    pipelines.append((name + '-orig', Pipeline(steps=[(name, model)])))\n",
    "    pipelines.append((name + '-padr', Pipeline(steps=[standard_scaler, (name, model)])))\n",
    "    pipelines.append((name + '-norm', Pipeline(steps=[min_max_scaler, (name, model)])))\n",
    "\n",
    "# Definindo os parâmetros para GridSearchCV\n",
    "param_grids = {\n",
    "    'LR': {\n",
    "        'LR__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'LR__solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'KNN': {\n",
    "        'KNN__n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21],\n",
    "        'KNN__metric': [\"euclidean\", \"manhattan\", \"minkowski\"]\n",
    "    },\n",
    "    'CART': {\n",
    "        'CART__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "        'CART__min_samples_split': [2, 5, 10],\n",
    "        'CART__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'NB': {\n",
    "        'NB__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'SVM__C': [0.1, 1, 10, 100],\n",
    "        'SVM__gamma': [1, 0.1, 0.01, 0.001],\n",
    "        'SVM__kernel': ['rbf', 'linear']\n",
    "    },\n",
    "    'RF': {\n",
    "        'RF__n_estimators': [10, 50, 100, 200],\n",
    "        'RF__max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'RF__max_depth': [None, 10, 20, 30],\n",
    "        'RF__min_samples_split': [2, 5, 10],\n",
    "        'RF__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'ET': {\n",
    "        'ET__n_estimators': [10, 50, 100, 200],\n",
    "        'ET__max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'ET__max_depth': [None, 10, 20, 30],\n",
    "        'ET__min_samples_split': [2, 5, 10],\n",
    "        'ET__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Ada': {\n",
    "        'Ada__n_estimators': [10, 50, 100, 200],\n",
    "        'Ada__learning_rate': [0.01, 0.1, 1, 10]\n",
    "    },\n",
    "    'GB': {\n",
    "        'GB__n_estimators': [10, 50, 100, 200],\n",
    "        'GB__learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "        'GB__max_depth': [3, 5, 7, 9]\n",
    "    },\n",
    "    'Voting': {\n",
    "        # Para VotingClassifier, geralmente não há hiperparâmetros para ajustar diretamente\n",
    "        # Ajustar os hiperparâmetros dos estimadores base individualmente se necessário\n",
    "    }\n",
    "}\n",
    "\n",
    "# Parâmetros de cross-validation e scoring\n",
    "scoring = 'accuracy'\n",
    "kfold = 5\n",
    "\n",
    "# Executando o GridSearchCV para cada pipeline\n",
    "for name, pipeline in pipelines:\n",
    "    model_type = name.split('-')[0]\n",
    "    if model_type in param_grids:\n",
    "        param_grid = param_grids[model_type]\n",
    "    else:\n",
    "        param_grid = {}  # Para modelos que não têm parâmetros definidos\n",
    "\n",
    "    grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "    grid.fit(X_train, y_train)\n",
    "    # Imprimindo a melhor configuração\n",
    "    print(\"Modelo: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_))\n",
    "    \n",
    "#Demora para rodar, mas é possível salvar o modelo treinado para uso posterior\n",
    "# with open('best_model.pkl', 'wb') as f:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning do KNN\n",
    "\n",
    "np.random.seed(7) # definindo uma semente global para este bloco\n",
    "\n",
    "pipelines = []\n",
    "\n",
    "# Definindo os componentes do pipeline\n",
    "knn = ('KNN', KNeighborsClassifier())\n",
    "standard_scaler = ('StandardScaler', StandardScaler())\n",
    "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
    "\n",
    "pipelines.append(('knn-orig', Pipeline(steps=[knn])))\n",
    "pipelines.append(('knn-padr', Pipeline(steps=[standard_scaler, knn])))\n",
    "pipelines.append(('knn-norm', Pipeline(steps=[min_max_scaler, knn])))\n",
    "\n",
    "param_grid = {\n",
    "    'KNN__n_neighbors': [1,3,5,7,9,11,13,15,17,19,21],\n",
    "    'KNN__metric': [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "}\n",
    "\n",
    "# Prepara e executa o GridSearchCV\n",
    "for name, model in pipelines:    \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "    grid.fit(X_train, y_train)\n",
    "    # imprime a melhor configuração\n",
    "    print(\"Sem tratamento de missings: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050de15",
   "metadata": {},
   "source": [
    "## Finalização do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do modelo com o conjunto de testes\n",
    "# Melhor modelo\n",
    "# RF-norm - Melhor: 0.783287 usando {'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__min_samples_leaf': 1, 'RF__min_samples_split': 2, 'RF__n_estimators': 50}\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "# Preparação do modelo\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train) # aplicação da normalização no conjunto de treino\n",
    "model = RandomForestClassifier(n_estimators=50, \n",
    "                               max_features='sqrt',\n",
    "                               min_samples_split=2,\n",
    "                               max_depth=10,\n",
    "                               min_samples_leaf=1)\n",
    "model.fit(rescaledX, y_train)\n",
    "\n",
    "# Estimativa da acurácia no conjunto de teste\n",
    "rescaledTestX = scaler.transform(X_test) # aplicação da normalização no conjunto de teste\n",
    "predictions = model.predict(rescaledTestX)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739f4bf",
   "metadata": {},
   "source": [
    "Rodando o modelo a partir de um pipeline com os hiperparâmetros otimizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=50, \n",
    "                               max_features='sqrt',\n",
    "                               min_samples_split=2,\n",
    "                               max_depth=10,\n",
    "                               min_samples_leaf=1)\n",
    "\n",
    "pipeline = Pipeline(steps=[('MinMaxScaler', MinMaxScaler()), ('RF', model)])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3bee5",
   "metadata": {},
   "source": [
    "## Salvando os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c610c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo\n",
    "model_filename = 'rf_diabetes_classifier.pkl'\n",
    "with open(\"../models/\"+model_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Salvando o scaler\n",
    "scaler_filename = 'minmax_scaler_diabetes.pkl'\n",
    "with open(\"../scalers/\"+scaler_filename, 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "    \n",
    "# Salvando o pipeline\n",
    "pipeline_filename = 'rf_diabetes_pipeline.pkl'\n",
    "with open(\"../pipelines/\"+pipeline_filename, 'wb') as file:\n",
    "    pickle.dump(pipeline, file)\n",
    "    \n",
    "    \n",
    "# Salvando X_test e y_test\n",
    "X_test_df = pd.DataFrame(X_test, columns=dataset.columns[:-1])\n",
    "y_test_df = pd.DataFrame(y_test, columns=[dataset.columns[-1]])\n",
    "X_test_df.to_csv(\"../data/addicted_score.csv\", index=False)\n",
    "y_test_df.to_csv(\"../data/students_social_media_data_sem_addicted_score.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988be820",
   "metadata": {},
   "source": [
    "## Simulando a aplicação do modelo em dados não vistos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02678e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação do modelo com TODO o dataset\n",
    "scaler = MinMaxScaler().fit(X) # ajuste do scaler com TODO o dataset\n",
    "rescaledX = scaler.transform(X) # aplicação da normalização com TODO o dataset\n",
    "model.fit(rescaledX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novos dados - não sabemos a classe!\n",
    "data = {'preg':  [1, 9, 5],\n",
    "        'plas': [90, 100, 110],\n",
    "        'pres': [50, 60, 50],\n",
    "        'skin': [30, 30, 30],\n",
    "        'test': [100, 100, 100],\n",
    "        'mass': [20.0, 30.0, 40.0],\n",
    "        'pedi': [1.0, 2.0, 1.0],\n",
    "        'age': [15, 40, 40],  \n",
    "        }\n",
    "\n",
    "atributos = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
    "entrada = pd.DataFrame(data, columns=atributos)\n",
    "\n",
    "array_entrada = entrada.values\n",
    "X_entrada = array_entrada[:,0:8].astype(float)\n",
    "\n",
    "# Padronização nos dados de entrada usando o scaler utilizado em X\n",
    "rescaledEntradaX = scaler.transform(X_entrada)\n",
    "print(rescaledEntradaX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9605f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predição de classes dos dados de entrada\n",
    "saidas = model.predict(rescaledEntradaX)\n",
    "print(saidas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
